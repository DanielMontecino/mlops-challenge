# MLOps Pipeline

This repository contains a training pipeline of a regression model and an API for deploying the model into production.

## Requirements

The only requirements to run the pipeline and the API is ```docker``` and ```docker-compose```.
In addition, to perform a final load test you should install locust:

```zh
pip3 install locust
```
## Overview

The model training pipeline was build using ```airflow```. Airflow which is an open-source workflow management platform for data engineering pipelines. Airflow uses directed acyclic graphs (DAGs) to manage workflow orchestration. 

A containerised version of Airflow is used through docker-compose.


The API that can take incoming predictions requests was build using ```FastAPI```, which is a Web framework for developing RESTful APIs in Python. This API was also build using Docker containers.

## Previous setup

### Clone the repository

The first step is clone the repository and entry to its root folder
```zh
git clone https://github.com/DanielMontecino/mlops-challenge
cd mlops-challenge
```

### Verify ports
FastApi uses the port 80 to receive request. Airflow uses ports 6379, 8080 and 5555, and locust uses port 8089. Therefore, the first step is to make sure that
these port are unused.

Show all the used ports with the following command, and look for the mentioned ports.
```zh
ss -ntl
```
or look for each port with the next command (if there is no responde, ports are unused):
```zh
ss -ntl | grep 80
ss -ntl | grep 8080
ss -ntl | grep 6379
ss -ntl | grep 5555
ss -ntl | grep 8089
```

## Training Pipeline

To run the pipeline, the first step is to run airflow.

### Initialize Airflow

To create necessary folders and initialize airflow, just run:

```zh
bash init.sh
```

If you are curious, this shell script create some folders, set an airflow environment variable, initialize airflow and run it.

```zh
#!/bin/bash

mkdir -p ./pipeline/logs ./pipeline/plugins ./data/raw_data ./data/processed_data ./models
echo -e "AIRFLOW_UID=$(id -u)" > ./pipeline/.env

cd pipeline
docker build -t airflow-custom .
docker-compose up airflow-init
docker-compose up
```
Wait until the page ```http://localhost:8080/home``` run correctly. 
To enter to the airflow page (```http://localhost:8080/home```), use the credentials:

```
user: airflow
pass: airflow
```

Once inside, go to the ml_pipeline marked in a red bbox bellow.

![Alt text](images/airflow_1.png "Airflow main page")

### The pipeline

The pipeline is shown below

![Alt text](images/pipeline.png "pipeline")

First, data extraction is performed. A module was created to extract data (get_data) that contains 3 sub modules
that are executed in parallel, one for each of the sets (precipitaciones, banco_central and precio_leche).

If the data extraction is successful, the processing module is triggered. Here, the 3 sets are processed, and the train and test datasets are generated.

The model is then trained using the training data and serialized in the ```models``` folder in .pk and .joblib format.

Finally, the model is tested using the test dataset. The evaluation generates RMSE and R2 metrics, although it could be
included any other metrics you want. In a CI/CD pipeline, at this stage it should be evaluated whether the model meets the requirements
to put into production, and if it is the case, trigger the model deployment. In this project, this step will be done manually.

In addition to the logs generated by airflow in the ```pipeline/logs folder``` (```dag_processor_manager```, ```ml_pipeline``` and ```scheduler```), the
```pipeline/logs/pipeline_logs``` file is created, where custom pipeline logs are saved.

### Run Pipeline

To run the pipeline, just press the play button marked in a red bbox in the next image.

![Alt text](images/airflow_2.png "run ml_pipeline")

If the run success, you should see green squares next to each module.

To verify that the process was correctly run, we could see if the data was downloaded correctly (it wasn't included in the original repo) and that the model was correctly saved.

```zh
ls data/raw_data
ls data/processed_data
ls models
```

And, of course, we can check the test metric in the logs.

```zh
cat pipeline/logs/pipeline_logs
```

## Model Deployment

